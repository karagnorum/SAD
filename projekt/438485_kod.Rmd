---
title: "438485_kod"
author: "Krzysztof Hajderek"
date: "2024-04-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Zadanie 1.
Wczytujemy dane:
```{r}
x_test <- read.csv("X_test.csv")
x_train <- read.csv("X_train.csv")
y <- read.csv("y_train.csv")
y <- y$CD36
# Czy dokonać jakiejś konwersji?
```

Sprawdzamy strukturę danych i czy nie ma braków:
```{r}
anyNA.data.frame(x_train)
anyNA.data.frame(x_test)
anyNA(y)
str(x_test)
str(x_train)
str(y)
```
Widzimy, że nie ma braków.

Podstawowe statystyki zmiennej objaśnianej:
```{r}
print(var(y))
summary(y)
```

Estymator gęstości:
```{r}
plot(density(y, bw = 'nrd'))
```
```{r}
cor_y = function(x) cor(x, y, method = "pearson")
cors = apply(x_train, 2, cor_y)
cors = sort(cors, decreasing = TRUE)
```
```{r}
most_correlated = x_train[names(cors)[1:250]]
```

Zadanie 2.
```{r}
qqnorm(y)
```
c) Wybieramy zmienną najbardziej skorelowaną ze zmienną objaśnianą i rysujemy wykres estymatora gęstości.
```{r}
chosen_x = x_train[,names(cors)[1]]
d = density(chosen_x, bw = 'nrd')
plot(d)
```
Dystrybunta empiryczna najbardziej przypomina rozkład normalny. Niech $H_0$: próbka pochodzi z pewnego rozkładu normalnego, $H_1$: $\sim H_0$. Zweryfikujemy hipotezę przy pomocy testu Andersona-Darlinga na poziomie istotności 0.01.
```{r}
library(nortest)
ad.test(chosen_x)
```
P - wartość jest poniżej 0.01, więc odrzucamy $H_0$.
Zadanie 3.
b)  Użyjemy 10-fold, aby uzyskać balans między minimalizowaniem wariancji i zwiększaniem elastyczności.
```{r}
library(glmnet)
x = data.matrix(x_train[1:100, 1:100])
t = data.matrix(x_test[1:100,1:100])
y_ = y[1:100]
nfolds = 10
foldid = sample(cut(1:nrow(x), nfolds, labels = F))
alphas = seq(from = 0, to = 1, by = 0.5)
lambdas = seq(from = 0.5, to = 1.5, by = 0.5)
new_array = function() array(dim = c(length(alphas), length(lambdas), nfolds))
validation_errors = new_array()
min_error = Inf
for (i in 1:length(alphas)){
  for (j in 1:length(lambdas)){
    test_errors = numeric(nfolds)
    for (k in 1:nfolds){
      train = which(foldid != k)
      model = glmnet(x[train,], y_[train], lambda = lambdas[j], alpha = alphas[i], family = "gaussian")
      predicted = predict(model, newx = x, type = "response")
      validation_errors[i, j, k] = mean((predicted[-train,]-y_[-train])^2)
      train_errors[k] = mean((predicted[train,]-y_[train])^2)
    }
    validation_error = mean(validation_errors[i,j,])
    if (validation_error < min_error){
        which_alpha_best = i
        which_lambda_best= j
        min_error = validation_error
        train_error = mean(train_errors) #train error in best model
      }
  }
}
alphas[which_alpha_best]
lambdas[which_lambda_best]
```
c)
```{r}
library(ggplot2)

alpha = new_array()
lambda = new_array()
fold = new_array()

for (i in 1:length(alphas)){
  for (j in 1:length(lambdas)){
    for (k in 1:nfolds){
      alpha[i, j,k] = alphas[i]
      lambda[i,j,k] = lambdas[j]
      fold[i, j, k] = k
    }
  }
}

al = as.vector(alpha)
la = as.vector(lambda)
params = character(length(al))
for (i in 1:length(al)){
  params[i] = paste(al[i], ", ", la[i])
}

dat = data.frame(
  parameters = factor(params),
  fold = as.vector(fold),
  mse = as.vector(validation_errors))

ggplot(dat, aes(x = parameters, y = mse)) +
  geom_violin(trim = FALSE) +
  labs(title = "Mean Squared Errors Across Folds and Hyperparameters",
       x = "Hyperparameters (Alpha, Lambda)", y = "Mean Squared Error") +
  theme_minimal()
```

d)
```{r}
train_error
mean(validation_errors[which_alpha_best, which_lambda_best,])
```
Zadanie 4.

